{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" />\n",
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" />\n",
    "\n",
    "<br><h2>Script 05 | Advanced Regression Models</h2>\n",
    "<h4>DAT-5390 | Computational Data Analytics with Python</h4>\n",
    "Chase Kusterer - Faculty of Analytics<br>\n",
    "Hult International Business School<br><br><br>\n",
    "\n",
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" />\n",
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "<h2>Comparing statsmodels and scikit-learn</h2><br>\n",
    "It may seem counterproductive to build models in both statsmodels and scikit-learn, but each package has its advantages.<br><br>\n",
    "<u>Advantages of statsmodels</u><br>\n",
    "\n",
    "* This is a great tool for generating model summaries, enabling analysts to base decisions on familiar metrics such as p-values and R-Square.\n",
    "* Model outputs are similar to that of R and Excel.\n",
    "<br><br>\n",
    "\n",
    "<u>Advantages of scikit-learn</u><br>\n",
    "\n",
    "* Minimal things happen behind the scenes, making scikit-learn faster than statsmodels. This becomes a serious advantage when running a model in real time on a server or cloud.\n",
    "* It is incredibly easy to change model types, allowing analysts to experiment with minimal effort.\n",
    "<br><br>\n",
    "\n",
    "<u>Disadvantages of statsmodels</u><br>\n",
    "\n",
    "* Oftentimes, a substantial amount of code needs to be modified in order to change model types.\n",
    "* Some metrics in a model's summary output may not be relevant to the the analysis at hand. Furthermore, the most important metrics for the analysis may not be available in the summary.\n",
    "<br><br>\n",
    "\n",
    "<u>Disadvantages of scikit-learn</u><br>\n",
    "\n",
    "* Analysts have to tell scikit-learn which metrics to generate, which can be tedious in a complex analysis.\n",
    "* Some statistical concepts, such as p-values, do not exist.\n",
    "\n",
    "<br>\n",
    "\n",
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "<h2>Part I: Preparation</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import pandas as pd                                  # data science essentials\n",
    "import numpy  as np                                  # mathematical essentials\n",
    "import matplotlib.pyplot as plt                      # data viz\n",
    "import seaborn as sns                                # enhanced data viz\n",
    "import statsmodels.formula.api as smf                # linear modeling\n",
    "from sklearn.model_selection import train_test_split # train/test split\n",
    "import sklearn.linear_model                          # faster linear modeling\n",
    "from baserush.optimize import quick_lm               # efficient base modeling\n",
    "from baserush.summary import  lr_summary             # New! model summaries\n",
    "\n",
    "\n",
    "# new libraries\n",
    "from sklearn.preprocessing import StandardScaler  # standard scaler\n",
    "import warnings                                   # warnings from code\n",
    "\n",
    "# setting pandas print options\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.float_format', lambda x: '%.2f' % x)\n",
    "\n",
    "\n",
    "# suppressing warnings\n",
    "warnings.filterwarnings(action = 'ignore')\n",
    "\n",
    "\n",
    "# specifying the path and file name\n",
    "file = './datasets/housing_feature_rich.xlsx'\n",
    "\n",
    "\n",
    "# reading the file into Python\n",
    "housing = pd.read_excel(io     = file,\n",
    "                        header = 0   )\n",
    "\n",
    "\n",
    "housing.drop(labels  = ['property_id'],\n",
    "             axis    = 1,\n",
    "             inplace = True)\n",
    "\n",
    "\n",
    "#####################################\n",
    "# importing model coefficients file #\n",
    "#####################################\n",
    "results_path = \"./model_results/model_results.xlsx\"\n",
    "\n",
    "results_df   = pd.read_excel(io     = results_path,\n",
    "                             header = 0           )\n",
    "\n",
    "\n",
    "\n",
    "# checking housing dataset\n",
    "housing.head(n = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" /><br>\n",
    "\n",
    "<h3>Candidate Models</h3><br>\n",
    "Run the following code to instantiate the candidate models from previous scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################\n",
    "## original data (full models) ##\n",
    "#################################\n",
    "# all x-data\n",
    "x_all = list(housing.drop(labels  = ['Sale_Price', 'log_Sale_Price'],\n",
    "                          axis    = 1))\n",
    "\n",
    "# continuous x-data\n",
    "x_original = list(housing.loc[ : , 'Lot_Area' : 'Porch_Area' ])\n",
    "\n",
    "\n",
    "\n",
    "################\n",
    "## original y ##\n",
    "################\n",
    "# best base model \n",
    "x_base = ['Mas_Vnr_Area',  'Total_Bsmt_SF', 'First_Flr_SF',\n",
    "          'Second_Flr_SF', 'Garage_Area']\n",
    "\n",
    "\n",
    "# best model after feature engineering\n",
    "x_step = ['Total_Bsmt_SF', 'Overall_Qual', 'NridgHt', 'Other_NH',\n",
    "          'Kitchen_AbvGr', 'Mas_Vnr_Area', 'has_Second_Flr', 'Total_Bath',\n",
    "          'Crawfor', 'Overall_Cond', 'NWAmes', 'Somerst', 'Second_Flr_SF',\n",
    "          'Fireplaces', 'Garage_Cars', 'has_Garage', 'First_Flr_SF',\n",
    "          'has_Mas_Vnr', 'OldTown', 'Porch_Area', 'CulDSac', 'CollgCr',\n",
    "          'has_Porch', 'ratio_building_lot']\n",
    "\n",
    "\n",
    "###################\n",
    "## logarithmic y ##\n",
    "###################\n",
    "# best model after feature engineering (log y)\n",
    "x_step_log_y = ['Gr_Liv_Area', 'Overall_Qual', 'Garage_Cars', 'Total_Bsmt_SF',\n",
    "                'log_Lot_Area', 'OldTown', 'Overall_Cond', 'log_Gr_Liv_Area',\n",
    "                'Kitchen_AbvGr', 'Total_Bath', 'has_Second_Flr',\n",
    "                'Second_Flr_SF', 'NridgHt', 'Fireplaces', 'NWAmes', 'Somerst',\n",
    "                'Porch_Area', 'CollgCr', 'Crawfor', 'First_Flr_SF', 'Edwards',\n",
    "                'CulDSac', 'm_Mas_Vnr_Area']\n",
    "\n",
    "\n",
    "########################\n",
    "## response variables ##\n",
    "########################\n",
    "original_y = 'Sale_Price'\n",
    "log_y      = 'log_Sale_Price'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating placeholder DataFrame for results\n",
    "results = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" /><br>\n",
    "\n",
    "Let's run the OLS models from the previous script so that we can compare them to more advanced models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## stepwise model using Sale_Price ##\n",
    "sp_model = quick_lm(x_data        = housing[ x_all ],\n",
    "                    y_data        = housing[ original_y ],\n",
    "                    threshold_in  = 0.01,\n",
    "                    threshold_out = 0.05,\n",
    "                    test_size     = 0.25,\n",
    "                    verbose       = False) # suppressing output\n",
    "\n",
    "\n",
    "# storing results\n",
    "results = lr_summary(x = housing[ sp_model['selected_features'] ],\n",
    "                     y = housing[ original_y ],\n",
    "                     model = sklearn.linear_model.LinearRegression(),\n",
    "                     model_name = 'OLS Model (y)',\n",
    "                     results_df = results)\n",
    "\n",
    "\n",
    "# checking results\n",
    "results.head(n = 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## stepwise model using Sale_Price ##\n",
    "sp_model = quick_lm(x_data        = housing[ x_all ],\n",
    "                    y_data        = housing[ log_y ],\n",
    "                    threshold_in  = 0.01,\n",
    "                    threshold_out = 0.05,\n",
    "                    test_size     = 0.25,\n",
    "                    verbose       = False)\n",
    "\n",
    "\n",
    "# storing results\n",
    "results = lr_summary(x = housing[ sp_model['selected_features'] ],\n",
    "                     y = housing[ log_y ],\n",
    "                     model = sklearn.linear_model.LinearRegression(),\n",
    "                     model_name = 'OLS Model (log y)',\n",
    "                     results_df = results)\n",
    "\n",
    "\n",
    "# checking results\n",
    "results.head(n = 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" /><br>\n",
    "\n",
    "<h2>Part II: Ridge Regression</h2><br>\n",
    "Ridge regression is a model type that has a shrinkage parameter. In other words, ridge models can tune each x-feature to make it more stable (more formally known as <strong>regularization</strong>). Think of stability as a coefficient from an OLS regression model that has a p-value exactly equal to zero. Therefore, instability would be a coefficient with a p-value greater than zero. Too much instability in a coefficient implies that an x-feature is insignificant, such as when a p-value gets above 0.05 (assuming 95% confidence). In OLS regression, we <strong>regulate</strong> a model by removing insignificant features. Note that mathematically, this is the same as setting the feature's coefficient to zero.\n",
    "<br><br>\n",
    "Now imagine a model that has the ability to shrink a feature's coefficient instead of setting it to zero. This is what ridge and other regularization models do. When a ridge model finds a coefficient that is unstable, it\n",
    "shrinks it until stability is achieved. This tends to lead to weaker predictive performance in terms of metrics like R-Square. However, this also tends to lead to greater stability, which can be observed through metrics like the train-test gap. <strong>Stable models are preferred to unstable models</strong> because they are more likely to perform as expected in the real world. Remember that even though a model may \"look\" good on paper, its job is to predict something that is currently unknown. There is less risk in getting unexpected results if a model is stable. \n",
    "<br><br>\n",
    "Here's a video if you'd like to <a href=\"https://www.youtube.com/watch?app=desktop&v=Q81RR3yKn30\">learn more about ridge regression</a>.<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(sklearn.linear_model.Ridge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "<strong>a)</strong> Complete the code below using x_all and log_y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing x-data\n",
    "x_data = housing[ _____ ]\n",
    "\n",
    "# preparing y-data\n",
    "y_data = housing[ _____ ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "<br><br><strong>a)</strong> Develop a ridge regression model with <strong>sklearn.linear_model.Ridge(&nbsp;)</strong>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiating a ridge model\n",
    "model = sklearn.linear_model._____(alpha        = 1.0,\n",
    "                                   random_state = 702)\n",
    "\n",
    "\n",
    "# analyzing results\n",
    "results = baserush.summary.lr_summary(x          = x_data,\n",
    "                                      y          = y_data,\n",
    "                                      model      = model,\n",
    "                                      model_name = 'Unscaled Ridge Model',\n",
    "                                      results_df = results)\n",
    "\n",
    "\n",
    "# checking results\n",
    "results.head(n = 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" /><br>\n",
    "<h2>Part III: Lasso Regression</h2><br>\n",
    "Next up is lasso regression, which is also a regulation model that is very similar to ridge regression. The major difference between these model types is that lasso can shrink a coefficient to zero, whereas ridge can only get extremely close to zero. This means that lasso models have a built-in variable selection technique: it can set coefficients to zero, which effectively kicks them out of the model. This can be very useful in the early stages of an analysis.\n",
    "<br><br>\n",
    "This video is a great way to <a href=\"https://www.youtube.com/watch?app=desktop&v=NGf0voTMlcs\">learn more about lasso regression</a>.<br><br>\n",
    "\n",
    "<img src=\"./script_images/lasso.png\" alt=\"Ted Lasso\" width=\"400\"/>\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(sklearn.linear_model.Lasso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "<br><strong>a)</strong> Develop a lasso regression model with <strong>sklearn.linear_model.Lasso(&nbsp;) </strong>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiating a ridge model\n",
    "model = sklearn.linear_model._____(alpha        = 1.0,\n",
    "                                   random_state = 702)\n",
    "\n",
    "\n",
    "# analyzing results\n",
    "results = lr_summary(x          = x_data,\n",
    "                     y          = y_data,\n",
    "                     model      = model,\n",
    "                     model_name = 'Unscaled Lasso Model',\n",
    "                     results_df = results)\n",
    "\n",
    "\n",
    "# checking results\n",
    "results.head(n = 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" /><br>\n",
    "<h2>Part IV: Stochastic Gradient Descent</h2><br>\n",
    "I have a very interesting memory from my childhood that I would like to share with you. When I was around five years old, I remember seeing a lot of soap commercials on TV. Each one would explain how their soap was the best, much better when compared to other leading brands. One would be the best at killing germs. Another would be the best at moisturizing. This one smells the best. That one is the most recommended by doctors, so it's the best. As I saw more commercials, I kept thinking: Why don't they just combine all the best soaps together? Wouldn't that lead to a soap that's the best at everything? There would be no more debate on this subject.\n",
    "<br><br>\n",
    "Later in life, I learned about ridge and lasso models. Both use different regularization techniques, which can lead to one model working better than the other depending on the data. So, how do you know when to use one over  the other? Wouldn't it be great if we could just combine them together like the soaps of my childhood? This is exactly what an <strong>elastic net</strong> does. If you'd like to know more, check out <a href=\"https://www.youtube.com/watch?app=desktop&v=1dKRdX9bfIo\">this video on elastic net regression</a>.\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(sklearn.linear_model.SGDRegressor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "<br><br><strong>a)</strong> Develop an SGD regression model with <strong>SGDRegressor(&nbsp;)</strong>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiating a ridge model\n",
    "model = sklearn.linear_model._____(loss     = 'squared_error',\n",
    "                                   penalty  = 'elasticnet',\n",
    "                                   alpha    = 0.001,\n",
    "                                   l1_ratio = 0.15,\n",
    "                                   random_state = 702)\n",
    "\n",
    "\n",
    "# analyzing results\n",
    "results = lr_summary(x          = x_data,\n",
    "                     y          = y_data,\n",
    "                     model      = model,\n",
    "                     model_name = 'Unscaled SGD Model',\n",
    "                     results_df = results)\n",
    "\n",
    "\n",
    "# checking results\n",
    "results.head(n = 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" /><br>\n",
    "\n",
    "<h2>Part V: Standardization</h2><br>\n",
    "In this section, we will learn how to <strong>standardize</strong> the X-features. In other words, we are going to put them into a form where each feature's variance is measured on the same scale. Some algorithms base their calculations on distance, which requires standardization so that they work properly. Others have penalty terms that assume all features have a mean of zero and a standard deviation of one,  which is the result of standardization.\n",
    "<br><br>\n",
    "In general, distance- and penalty-based algorithms (like K-Nearest Neighbors, which we will see in the next script) perform much better after standardization. This is because distance-based algorithms use variance to compute similarity amongst observations: the closer two observations are in terms of their variance, the more similar the algorithm will think they are. Therefore, if the data is not standardized, features with less variance may take over the model. This can be a lot to conceptualize, so let's take it step by step, keeping in mind that our goal is to ensure that the variance in each feature is treated fairly by the algorithms we develop.\n",
    "<br><br><strong>Standard Scaler</strong><br>\n",
    "Technically speaking, this is our first unsupervised learning technique! Congrats on all that you've accomplished thus far! Notice how the process of data standardization is very similar to that of building models in scikit-learn:<br>\n",
    "\n",
    "* Instantiate\n",
    "* Fit\n",
    "* <strike>Predict</strike> Transform\n",
    "* <strike>Score</strike> Convert\n",
    "\n",
    "<br>\n",
    "<strong>a)</strong> Complete the code below to standardize all X-features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTANTIATING a StandardScaler() object\n",
    "scaler = StandardScaler()\n",
    "\n",
    "\n",
    "# FITTING and TRANSFORMING\n",
    "x_scaled = scaler.fit_transform( _____ )\n",
    "\n",
    "\n",
    "# converting scaled data into a DataFrame\n",
    "x_scaled_df = pd.DataFrame(x_scaled)\n",
    "\n",
    "\n",
    "# labeling columns\n",
    "x_scaled_df.columns = _____.columns\n",
    "\n",
    "\n",
    "# checking the results\n",
    "x_scaled_df.describe(include = 'number').round(decimals = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "<br><br>\n",
    "<h3>Standardizing Candidate Models</h3><br>\n",
    "Now that we've instantiated a standardized version of the x-data, we can standardize each candidate model with subsetting, as exemplified in the two codes below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "# x_all (not standardized)\n",
    "housing[x_all].iloc[ : , 0:3 ].head(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "# x_all (standardized)\n",
    "x_scaled_df[x_all].iloc[ : , 0:3 ].head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "<br><hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" /><br>\n",
    "<h2>Part VI: Team Challenge</h2>\n",
    "<br>Below are the available candidate models.\n",
    "<br><br>\n",
    "<u><strong>Candidate Models (X-features)</strong></u>\n",
    "\n",
    "* x_all\n",
    "* x_original\n",
    "* x_base\n",
    "* x_step\n",
    "* x_step_log_y\n",
    "\n",
    "\n",
    "<br>\n",
    "<u><strong>Response Variables (y)</strong></u>\n",
    "\n",
    "* original_y\n",
    "* log_y\n",
    "\n",
    "\n",
    "<br>Your objectives in this challenge are to:\n",
    "\n",
    "1. Determine which models perform better with standardized data.\n",
    "2. Choose the best candidate model for each of the following model types:\n",
    "\n",
    "* <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\">OLS Regression</a>\n",
    "* <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html\">Ridge Regression</a>\n",
    "* <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html\">Lasso Regression</a>\n",
    "* <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html\">SGD Regression</a>\n",
    "\n",
    "<br>\n",
    "<strong>a)</strong> Run each candidate model on each model type, recording its results in <em>results_df</em>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing x-data\n",
    "x_data =  _____ [ _____ ] # df can be housing or x_scaled_df\n",
    "\n",
    "# preparing y-data\n",
    "y_data = housing[ _____ ] # df can only be housing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiating a ridge model\n",
    "model = sklearn.linear_model._____(loss     = 'squared_error',\n",
    "                                   penalty  = 'elasticnet',\n",
    "                                   alpha    = 0.001,\n",
    "                                   l1_ratio = 0.15,\n",
    "                                   random_state = 702)\n",
    "\n",
    "\n",
    "# analyzing results\n",
    "results = lr_summary(x          = x_data,\n",
    "                     y          = y_data,\n",
    "                     model      = model,\n",
    "                     model_name = _____,\n",
    "                     results_df = results)\n",
    "\n",
    "\n",
    "# checking results\n",
    "results.head(n = 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "<br>\n",
    "<h3>Analysis</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this markdown cell to write your analysis. Good luck!\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" /><br>\n",
    "\n",
    "~~~\n",
    "\n",
    "\n",
    " █████╗ ██╗     ██╗ ██████╗ ███╗   ██╗██╗███╗   ██╗ ██████╗ \n",
    "██╔══██╗██║     ██║██╔════╝ ████╗  ██║██║████╗  ██║██╔════╝ \n",
    "███████║██║     ██║██║  ███╗██╔██╗ ██║██║██╔██╗ ██║██║  ███╗\n",
    "██╔══██║██║     ██║██║   ██║██║╚██╗██║██║██║╚██╗██║██║   ██║\n",
    "██║  ██║███████╗██║╚██████╔╝██║ ╚████║██║██║ ╚████║╚██████╔╝\n",
    "╚═╝  ╚═╝╚══════╝╚═╝ ╚═════╝ ╚═╝  ╚═══╝╚═╝╚═╝  ╚═══╝ ╚═════╝ \n",
    "                                                            \n",
    "██╗    ██╗██╗████████╗██╗  ██╗                              \n",
    "██║    ██║██║╚══██╔══╝██║  ██║                              \n",
    "██║ █╗ ██║██║   ██║   ███████║                              \n",
    "██║███╗██║██║   ██║   ██╔══██║                              \n",
    "╚███╔███╔╝██║   ██║   ██║  ██║                              \n",
    " ╚══╝╚══╝ ╚═╝   ╚═╝   ╚═╝  ╚═╝                              \n",
    "                                                            \n",
    "███████╗██╗   ██╗ ██████╗ ██████╗███████╗███████╗███████╗██╗\n",
    "██╔════╝██║   ██║██╔════╝██╔════╝██╔════╝██╔════╝██╔════╝██║\n",
    "███████╗██║   ██║██║     ██║     █████╗  ███████╗███████╗██║\n",
    "╚════██║██║   ██║██║     ██║     ██╔══╝  ╚════██║╚════██║╚═╝\n",
    "███████║╚██████╔╝╚██████╗╚██████╗███████╗███████║███████║██╗\n",
    "╚══════╝ ╚═════╝  ╚═════╝ ╚═════╝╚══════╝╚══════╝╚══════╝╚═╝\n",
    "                                                            \n",
    "\n",
    "\n",
    "~~~\n",
    "\n",
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "<br>\n",
    "<h2>Bonus: Storing Model Results</h2><br>\n",
    "The code below will store the model results from above as a new Excel file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving results in Excel\n",
    "results_df.to_excel(excel_writer = \"./model_results/model_results_2.xlsx\",\n",
    "                    index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
